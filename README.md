## âœ‹ Gesture Recognition with biLSTM
A simple pipeline for gesture recognition using MediaPipe and a trained biLSTM model. This project includes data generation, preprocessing, live demonstration, and recovery video creation.

## ðŸ“¦ Project Structure :
```bash
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ bilstm_jsonl_best_model.pth              # Pretrained biLSTM model
â”œâ”€â”€ MediaPipe.py           # Generate hand gesture data using MediaPipe
â”œâ”€â”€ Convert.py             # Convert generated data to optimized JSONL format
â”œâ”€â”€ Recovery.py            # Create short gesture recovery videos
â”œâ”€â”€ Livedemo.py            # Run live demo with trained biLSTM model
```
## ðŸ“š Installation :
pip install -r requirements.txt

## ðŸš€ Usage :
1. Generate Data from Webcam (MediaPipe)
```bash
python MediaPipe.py
```
This script uses MediaPipe to record hand gestures and save keypoint data.

2. Convert Data Format (for Training)
```bash
python Convert.py
```
This will optimize data and convert it into JSONL format for training with Google Colab.

3. Live Demo with Pretrained biLSTM
```bash
python Livedemo.py
```
Uses bilstm_jsonl_best_model.pth to perform real-time gesture recognition from webcam input.

4. Create Recovery Videos
```bash
python Recover.py --session {directory to a file needed to recover}
```
## ðŸ§  Model Info :
Model: biLSTM

File: bilstm_jsonl_best_model.pth

Input: Hand keypoints (via MediaPipe)

Output: Gesture class label


